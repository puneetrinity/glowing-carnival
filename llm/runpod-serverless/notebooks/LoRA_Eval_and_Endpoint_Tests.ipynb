{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Evaluation & Endpoint Tests (Colab Ready)\n",
    "\n",
    "Evaluate a trained LoRA or merged FP16 model on HR domains (resume, JD, match, recruiting, ATS), plus router/safety and endpoint checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pinned deps (recommended on Colab)\n",
    "# If already installed, you can skip this cell.\n",
    "!pip -q install --no-deps 'xformers<0.0.27' 'trl<0.9.0' peft accelerate bitsandbytes\n",
    "!pip -q install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32=True\n",
    "torch.backends.cudnn.allow_tf32=True\n",
    "print('✓ Deps ready, CUDA:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Paths\n",
    "Set the model source: merged FP16 directory OR base + LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one setup:\n",
    "USE_MERGED = True  # True -> load merged FP16 dir; False -> load base + LoRA adapter\n",
    "MERGED_MODEL_DIR = '/content/colab_training/qwen2.5-7b-career-hr-merged'  # update if needed\n",
    "BASE_MODEL = 'unsloth/Qwen2.5-7B-Instruct-bnb-4bit'\n",
    "ADAPTER_DIR = '/content/colab_training/qwen2.5-7b-career-hr-final'  # folder that contains adapter config\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Config -> USE_MERGED:', USE_MERGED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "Loads merged FP16 or base+LoRA (Unsloth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "if USE_MERGED:\n",
    "    print('Loading merged FP16 model...')\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MERGED_MODEL_DIR,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=False,\n",
    "    )\n",
    "else:\n",
    "    print('Loading base + LoRA adapter...')\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=BASE_MODEL,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    # If adapter saved via model.save_pretrained, FastLanguageModel.get_peft_model\n",
    "    # will reconfigure with LoRA weights when you resumed training.\n",
    "    # If you have a separate adapter path, load via PEFT APIs as needed.\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "print('✓ Model ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validators (import or fallback)\n",
    "Imports VALIDATORS from audit_dataset.py if available; else defines minimal HR validators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, os, sys\n",
    "VALIDATORS=None\n",
    "try:\n",
    "    sys.path.insert(0, '/content/colab_training')\n",
    "    from audit_dataset import VALIDATORS as _VALS\n",
    "    VALIDATORS = _VALS\n",
    "    print('✓ Imported VALIDATORS from audit_dataset.py')\n",
    "except Exception as e:\n",
    "    print('⚠ Using fallback validators:', e)\n",
    "    def v_resume(text):\n",
    "        issues=[]; wc=len(text.split())\n",
    "        if wc<50 or wc>250: issues.append('len')\n",
    "        if not re.search(r'^\\s*[\\d\\-\\*\\•]', text, re.M): issues.append('bullets')\n",
    "        verbs=['led','built','developed','managed','created','improved','optimized','designed','implemented','achieved']\n",
    "        if not any(v in text.lower() for v in verbs): issues.append('verbs')\n",
    "        if ('ats keywords' not in text.lower()) and text.count(',')<5: issues.append('ats_line')\n",
    "        return (len(issues)==0), issues\n",
    "    def v_jd(text):\n",
    "        issues=[]; wc=len(text.split())\n",
    "        if wc<100 or wc>300: issues.append('len')\n",
    "        bullets=len(re.findall(r'^\\s*[\\d\\-\\*\\•]', text, re.M))\n",
    "        if bullets<8: issues.append('bullets')\n",
    "        if sum(s in text.lower() for s in ['responsibilities','requirements'])<2: issues.append('sections')\n",
    "        return (len(issues)==0), issues\n",
    "    def v_match(text):\n",
    "        issues=[]; wc=len(text.split())\n",
    "        if wc<80: issues.append('len')\n",
    "        if not re.search(r'\\b\\d{1,3}\\s*%|\\bscore[:\\s]+\\d{1,3}', text, re.I): issues.append('score')\n",
    "        if sum(t in text.lower() for t in ['match','gap','skill'])<2: issues.append('content')\n",
    "        return (len(issues)==0), issues\n",
    "    def v_recruit(text):\n",
    "        issues=[]; wc=len(text.split())\n",
    "        if wc<60: issues.append('len')\n",
    "        if sum(c in text.lower() for c in ['linkedin','referral','meetup','university','conference','github','stackoverflow'])<3: issues.append('channels')\n",
    "        if not any(w in text.lower() for w in ['week','month','quarter','daily','regularly','annually']): issues.append('cadence')\n",
    "        return (len(issues)==0), issues\n",
    "    def v_ats(text):\n",
    "        k=max(text.count(',')+1, len(re.findall(r'^\\s*[\\d\\-\\*\\•]', text, re.M)))\n",
    "        return (k>=15), ([] if k>=15 else ['count'])\n",
    "    VALIDATORS={\n",
    "        'resume_guidance': v_resume,\n",
    "        'job_description': v_jd,\n",
    "        'job_resume_match': v_match,\n",
    "        'recruiting_strategy': v_recruit,\n",
    "        'ats_keywords': v_ats,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Builders (ChatML) + 1‑shot for resume/JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_SEEDED = {'resume_guidance','recruiting_strategy','ats_keywords'}\n",
    "RESUME_EXAMPLE = (\n",
    "    '1. Led migration of 50+ services to Kubernetes, reducing deployments by 60%\\n'\n",
    "    '2. Built CI/CD with Jenkins + ArgoCD, enabling 20 daily deploys\\n'\n",
    "    '3. Optimized AWS costs by 35% via auto-scaling and rightsizing\\n'\n",
    "    '4. Designed REST APIs handling 10k+ req/sec with FastAPI\\n'\n",
    "    '5. Implemented monitoring with Prometheus/Grafana and on-call rota\\n'\n",
    "    '6. Mentored 3 junior engineers in code reviews and design\\n'\n",
    "    'ATS Keywords: Kubernetes, Docker, Jenkins, ArgoCD, Terraform, AWS, FastAPI, Python, CI/CD, Prometheus'\n",
    ")\n",
    "JD_EXAMPLE = (\n",
    "    '**Summary:** Experienced backend engineer to build scalable APIs.\\n'\n",
    "    '**Responsibilities:**\\n1. Design REST APIs\\n2. Optimize DB queries\\n3. Implement security\\n4. Collaborate with DevOps\\n5. Code reviews\\n6. Monitor SLIs/SLOs\\n'\n",
    "    '**Requirements:**\\n1. 5+ yrs backend\\n2. Python/Go/Node\\n3. SQL/NoSQL\\n4. AWS/GCP\\n5. System design\\n6. Communication\\n'\n",
    ")\n",
    "def build_chatml(domain, user):\n",
    "    if domain=='resume_guidance':\n",
    "        sys = 'You are a career coach specializing in resumes. Provide 6-8 bullets using action verbs. End with: \"ATS Keywords: ...\".\\n\\nExample:\\n'+RESUME_EXAMPLE\n",
    "    elif domain=='job_description':\n",
    "        sys = 'You are an HR specialist. Create JD with Summary, Responsibilities (6-8), Requirements (6-8).\\n\\nExample:\\n'+JD_EXAMPLE\n",
    "    elif domain=='job_resume_match':\n",
    "        sys = 'You are a technical recruiter. Provide: Score (0-100), Matches (5+), Gaps (3+), Next steps (3+).'\n",
    "    elif domain=='recruiting_strategy':\n",
    "        sys = 'You are a talent acquisition specialist. Provide channels (4+), cadence, metrics (100-150 words).'\n",
    "    else:\n",
    "        sys = 'You are a resume optimization specialist. Provide 20-40 ATS keywords comma-separated (80-120 words).'\n",
    "    seed = '1. ' if domain in HR_SEEDED else ''\n",
    "    return f"<|im_start|>system\\n{sys}<|im_end|>\\n<|im_start|>user\\n{user}<|im_end|>\\n<|im_start|>assistant\\n{seed}""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Suites (Local Generation)\n",
    "Runs N prompts per domain; validates with VALIDATORS; prints pass rates and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "  'resume_guidance': [\n",
    "    'Optimize my resume for Senior Backend Engineer (Python, FastAPI, AWS).',\n",
    "    'Create resume bullets for DevOps Lead (Kubernetes, Terraform, EKS).',\n",
    "    'Write resume bullets for Data Scientist (PyTorch, XGBoost, SQL).',\n",
    "    'Resume for Cloud Architect (AWS, IaC, cost optimization, security).',\n",
    "    'Resume for Frontend Lead (React, TypeScript, GraphQL, performance).',\n",
    "  ],\n",
    "  'job_description': [\n",
    "    'Write a JD for Senior DevOps Engineer at a cloud infra company.',\n",
    "    'Create a JD for Senior Backend Engineer at a fintech startup.'\n",
    "  ],\n",
    "  'job_resume_match': [\n",
    "    'Score match: 5 yrs React vs Senior Frontend role (React, TS, GraphQL).',\n",
    "    'Evaluate fit: DevOps (Docker, Jenkins) vs Cloud Architect (AWS, IaC).'\n",
    "  ],\n",
    "  'recruiting_strategy': [\n",
    "    'Sourcing strategy to hire 5 senior ML engineers in 3 months.',\n",
    "    'Recruiting plan for Security Engineers with cloud expertise.'\n",
    "  ],\n",
    "  'ats_keywords': [\n",
    "    'List ATS keywords for Senior Backend Engineer (Python, FastAPI, Postgres).',\n",
    "    'ATS terms for DevOps Engineer resume (Kubernetes, Terraform, CI/CD).'\n",
    "  ],\n",
    "}\n",
    "\n",
    "def run_local_tests(temperature=0.3, max_new_tokens=200):\n",
    "    results = {}\n",
    "    for domain, plist in PROMPTS.items():\n",
    "        passed=0; total=0; failures=[]\n",
    "        for p in plist:\n",
    "            prompt = build_chatml(domain, p)\n",
    "            x = tokenizer([prompt], return_tensors='pt').to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(**x, max_new_tokens=max_new_tokens, temperature=temperature, do_sample=False)\n",
    "            text = tokenizer.decode(out[0], skip_special_tokens=False)\n",
    "            try:\n",
    "                resp = text.split('<|im_start|>assistant\n',1)[1].split('<|im_end|>')[0].strip()\n",
    "            except Exception:\n",
    "                resp = ''\n",
    "            total += 1\n",
    "            ok, issues = VALIDATORS[domain](resp)\n",
    "            if ok:\n",
    "                passed += 1\n",
    "            else:\n",
    "                failures.append({'prompt': p, 'issues': issues, 'preview': resp[:160]})\n",
    "        rate = passed/total*100 if total else 0.0\n",
    "        results[domain] = {'passed': passed, 'total': total, 'rate': rate, 'failures': failures}\n",
    "    return results\n",
    "\n",
    "res = run_local_tests()\n",
    "for d, r in res.items():\n",
    "    print(f"{d:22s}: {r['passed']:2d}/{r['total']:2d} = {r['rate']:5.1f}%")\n",
    "    if r['failures']:\n",
    "        print('  Fails:', r['failures'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint Tests (optional)\n",
    "Set ENDPOINT_ID and API_KEY to validate deployed handler outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "ENDPOINT_ID = ''  # e.g., 'abcd1234'\n",
    "API_KEY = ''      # RunPod API key\n",
    "BASE = f'https://api.runpod.ai/v2/{ENDPOINT_ID}/runsync' if ENDPOINT_ID else None\n",
    "\n",
    "def call_endpoint(prompt, max_tokens=200, enable_validation=True):\n",
    "    if not BASE: return None\n",
    "    body = {\n",
    "        'input': {\n",
    "            'prompt': prompt,\n",
    "            'sampling_params': {'max_tokens': max_tokens, 'temperature': 0.3},\n",
    "            'enable_validation': enable_validation\n",
    "        }\n",
    "    }\n",
    "    headers={'Authorization': f'Bearer {API_KEY}', 'Content-Type':'application/json'}\n",
    "    r = requests.post(BASE, headers=headers, json=body, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "if BASE:\n",
    "    print('Testing endpoint...')\n",
    "    j = call_endpoint('Optimize my resume for Senior Backend Engineer (Python, FastAPI, AWS).')\n",
    "    out = (j or {}).get('output', {})\n",
    "    text = ((out.get('choices') or [{}])[0]).get('text','')\n",
    "    ok, issues = VALIDATORS['resume_guidance'](text)\n",
    "    print('Resume valid:', ok, 'issues:', issues)\n",
    "else:\n",
    "    print('Skipping endpoint tests (set ENDPOINT_ID & API_KEY to enable).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

