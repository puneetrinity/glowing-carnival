{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Evaluation & Endpoint Full Suite (Colab Ready)\n",
    "\n",
    "Run comprehensive local generation tests (100+ HR prompts), router/safety endpoint checks (salary blocking, non-career routing), and export CSV reports to Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pinned deps for stability on Colab\n",
    "!pip -q install --no-deps 'xformers<0.0.27' 'trl<0.9.0' peft accelerate bitsandbytes\n",
    "!pip -q install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'\n",
    "import torch, os\n",
    "torch.backends.cuda.matmul.allow_tf32=True\n",
    "torch.backends.cudnn.allow_tf32=True\n",
    "print('✓ Deps ready | CUDA:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model Paths\n",
    "Set merged FP16 model directory or base+LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MERGED = True  # True: merged FP16 dir | False: base + LoRA adapter\n",
    "MERGED_MODEL_DIR = '/content/colab_training/qwen2.5-7b-career-hr-merged'  # update if needed\n",
    "BASE_MODEL = 'unsloth/Qwen2.5-7B-Instruct-bnb-4bit'\n",
    "ADAPTER_DIR = '/content/colab_training/qwen2.5-7b-career-hr-final'  # optional if loading adapter\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Config:', {'USE_MERGED': USE_MERGED, 'DEVICE': DEVICE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "if USE_MERGED:\n",
    "    print('Loading merged FP16...')\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MERGED_MODEL_DIR, max_seq_length=2048, dtype=None, load_in_4bit=False)\n",
    "else:\n",
    "    print('Loading base + LoRA 4-bit...')\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=BASE_MODEL, max_seq_length=2048, dtype=None, load_in_4bit=True)\n",
    "FastLanguageModel.for_inference(model)\n",
    "print('✓ Model ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Validators (fallback if missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re, json\n",
    "VALIDATORS=None\n",
    "try:\n",
    "    sys.path.insert(0, '/content/colab_training')\n",
    "    from audit_dataset import VALIDATORS as _VALS\n",
    "    VALIDATORS = _VALS\n",
    "    print('✓ VALIDATORS imported')\n",
    "except Exception as e:\n",
    "    print('⚠ Fallback validators:', e)\n",
    "    def v_resume(text):\n",
    "        issues=[]; wc=len(text.split())\n",
    "        if wc<50 or wc>250: issues.append('len')\n",
    "        if not re.search(r'^\\s*[\\d\\-\\*\\•]', text, re.M): issues.append('bullets')\n",
    "        verbs=['led','built','developed','managed','created','improved','optimized','designed','implemented','achieved']\n",
    "        if not any(v in text.lower() for v in verbs): issues.append('verbs')\n",
    "        if ('ats keywords' not in text.lower()) and text.count(',')<5: issues.append('ats_line')\n",
    "        return (len(issues)==0), issues\n",
    "    def v_jd(text):\n",
    "        issues=[]; wc=len(text.split())\n",
    "        if wc<100 or wc>300: issues.append('len')\n",
    "        bullets=len(re.findall(r'^\\s*[\\d\\-\\*\\•]', text, re.M))\n",
    "        if bullets<8: issues.append('bullets')\n",
    "        if sum(s in text.lower() for s in ['responsibilities','requirements'])<2: issues.append('sections')\n",
    "        return (len(issues)==0), issues\n",
    "    def v_match(text):\n",
    "        issues=[]; wc=len(text.split())\n",
    "        if wc<80: issues.append('len')\n",
    "        if not re.search(r'\\b\\d{1,3}\\s*%|\\bscore[:\\s]+\\d{1,3}', text, re.I): issues.append('score')\n",
    "        if sum(t in text.lower() for t in ['match','gap','skill'])<2: issues.append('content')\n",
    "        return (len(issues)==0), issues\n",
    "    def v_recruit(text):\n",
    "        issues=[]; wc=len(text.split())\n",
    "        if wc<60: issues.append('len')\n",
    "        if sum(c in text.lower() for c in ['linkedin','referral','meetup','university','conference','github','stackoverflow'])<3: issues.append('channels')\n",
    "        if not any(w in text.lower() for w in ['week','month','quarter','daily','regularly','annually']): issues.append('cadence')\n",
    "        return (len(issues)==0), issues\n",
    "    def v_ats(text):\n",
    "        k=max(text.count(',')+1, len(re.findall(r'^\\s*[\\d\\-\\*\\•]', text, re.M)))\n",
    "        return (k>=15), ([] if k>=15 else ['count'])\n",
    "    VALIDATORS={\n",
    "        'resume_guidance': v_resume,\n",
    "        'job_description': v_jd,\n",
    "        'job_resume_match': v_match,\n",
    "        'recruiting_strategy': v_recruit,\n",
    "        'ats_keywords': v_ats,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Builders (ChatML) with 1‑shot for Resume/JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_SEEDED = {'resume_guidance','recruiting_strategy','ats_keywords'}\n",
    "RESUME_EXAMPLE = (\n",
    "    '1. Led migration of 50+ services to Kubernetes, reducing deployments by 60%\\n'\n",
    "    '2. Built CI/CD with Jenkins + ArgoCD, enabling 20 daily deploys\\n'\n",
    "    '3. Optimized AWS costs by 35% via auto-scaling and rightsizing\\n'\n",
    "    '4. Designed REST APIs handling 10k+ req/sec with FastAPI\\n'\n",
    "    '5. Implemented monitoring with Prometheus/Grafana and on-call rota\\n'\n",
    "    '6. Mentored 3 junior engineers in code reviews and design\\n'\n",
    "    'ATS Keywords: Kubernetes, Docker, Jenkins, ArgoCD, Terraform, AWS, FastAPI, Python, CI/CD, Prometheus'\n",
    ")\n",
    "JD_EXAMPLE = (\n",
    "    '**Summary:** Experienced backend engineer to build scalable APIs.\\n'\n",
    "    '**Responsibilities:**\\n1. Design REST APIs\\n2. Optimize DB queries\\n3. Implement security\\n4. Collaborate with DevOps\\n5. Code reviews\\n6. Monitor SLIs/SLOs\\n'\n",
    "    '**Requirements:**\\n1. 5+ yrs backend\\n2. Python/Go/Node\\n3. SQL/NoSQL\\n4. AWS/GCP\\n5. System design\\n6. Communication\\n'\n",
    ")\n",
    "def build_chatml(domain, user):\n",
    "    if domain=='resume_guidance':\n",
    "        sys = 'You are a career coach specializing in resumes. Provide 6-8 bullets using action verbs. End with: \\"ATS Keywords: ...\\".\\n\\nExample:\\n'+RESUME_EXAMPLE\n",
    "    elif domain=='job_description':\n",
    "        sys = 'You are an HR specialist. Create JD with Summary, Responsibilities (6-8), Requirements (6-8).\\n\\nExample:\\n'+JD_EXAMPLE\n",
    "    elif domain=='job_resume_match':\n",
    "        sys = 'You are a technical recruiter. Provide: Score (0-100), Matches (5+), Gaps (3+), Next steps (3+).'\n",
    "    elif domain=='recruiting_strategy':\n",
    "        sys = 'You are a talent acquisition specialist. Provide channels (4+), cadence, metrics (100-150 words).'\n",
    "    else:\n",
    "        sys = 'You are a resume optimization specialist. Provide 20-40 ATS keywords comma-separated (80-120 words).'\n",
    "    seed = '1. ' if domain in HR_SEEDED else ''\n",
    "    return f"<|im_start|>system\\n{sys}<|im_end|>\\n<|im_start|>user\\n{user}<|im_end|>\\n<|im_start|>assistant\\n{seed}"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Prompt Matrix (100+ HR prompts) & Router Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hr_prompts():\n",
    "    roles = ['Backend Engineer','DevOps Engineer','Data Scientist','ML Engineer','Cloud Architect',\n",
    "             'Frontend Engineer','SRE','Security Engineer','Platform Engineer','Mobile Engineer',\n",
    "             'Data Engineer','MLOps Engineer','Full Stack Engineer','Android Engineer','iOS Engineer',\n",
    "             'QA Automation Engineer','Release Engineer','Site Reliability Engineer','Product Engineer','Systems Engineer']\n",
    "    stacks = ['Python, FastAPI, Postgres, AWS','Kubernetes, Terraform, EKS','PyTorch, XGBoost, SQL',\n",
    "              'AWS, IaC, cost optimization','React, TypeScript, GraphQL','SLIs/SLOs, on-call, observability',\n",
    "              'IAM, KMS, threat modeling','K8s, GitOps, ArgoCD','Kotlin, Jetpack, CI/CD','Swift, SwiftUI, CI/CD']\n",
    "    # Build 20 resume prompts\n",
    "    resume = [f\"Optimize my resume for {role} ({stacks[i%len(stacks)]}).\" for i, role in enumerate(roles)]\n",
    "    resume = resume[:20]\n",
    "    # JD prompts\n",
    "    jd_roles = roles[:20]\n",
    "    jd = [f\"Write a job description for Senior {r} at a product company.\" for r in jd_roles]\n",
    "    # Match prompts\n",
    "    match = [\n",
    "        'Score match: 5 yrs React vs Senior Frontend role (React, TS, GraphQL).',\n",
    "        'Evaluate fit: DevOps (Docker, Jenkins) vs Cloud Architect (AWS, IaC).',\n",
    "        'Match analysis: Data Analyst vs Data Scientist (Python, SQL, ML).',\n",
    "        'Score: 7 yrs Java vs Senior Backend role (Spring, Microservices).',\n",
    "        'Evaluate: SRE background vs Platform Engineer (K8s, GitOps).',\n",
    "        'Score: Security Engineer vs AppSec Lead (threat modeling, SDLC).',\n",
    "        'Match: Mobile dev vs Cross-platform engineer (Flutter/React Native).',\n",
    "        'Evaluate: Full stack vs Backend specialist role (APIs, DB, perf).',\n",
    "        'Score: Cloud engineer vs Solutions Architect (AWS, IaC, networking).',\n",
    "        'Match: QA automation vs SDET (Selenium, PyTest, CI/CD).',\n",
    "    ]\n",
    "    # Recruiting prompts\n",
    "    recruiting = [\n",
    "        'Sourcing strategy to hire 5 senior ML engineers in 3 months.',\n",
    "        'Recruiting plan for Security Engineers with cloud expertise.',\n",
    "        'Hiring approach for building a data science team from scratch.',\n",
    "        'Sourcing strategy for senior DevOps in competitive market.',\n",
    "        'Recruiting plan for Frontend Leads with performance expertise.',\n",
    "        'Hiring plan for Platform Engineers in remote-first team.',\n",
    "        'Sourcing for Engineering Managers with startup experience.',\n",
    "        'Recruiting strategy for Mobile Engineers across Android/iOS.',\n",
    "        'Hiring plan for Full Stack Engineers at early-stage startup.',\n",
    "        'Sourcing strategy for Cloud Architects with enterprise background.'\n",
    "    ]\n",
    "    # ATS keywords prompts\n",
    "    ats = [\n",
    "        'List ATS keywords for Senior Backend Engineer (Python, FastAPI, Postgres).',\n",
    "        'ATS terms for DevOps Engineer resume (Kubernetes, Terraform, CI/CD).',\n",
    "        'ATS keywords for Data Scientist (PyTorch, MLOps, SQL).',\n",
    "        'List ATS terms for Cloud Architect (AWS, IaC, security).',\n",
    "        'ATS keywords for Frontend Lead (React, TypeScript, performance).',\n",
    "        'ATS terms for SRE (SLIs, SLOs, reliability, incident response).',\n",
    "        'ATS keywords for Security Engineer (IAM, KMS, threat modeling).',\n",
    "        'ATS list for Platform Engineer (K8s, GitOps, ArgoCD).',\n",
    "        'ATS terms for Mobile Developer (Kotlin, Swift, CI/CD).',\n",
    "        'ATS list for Full Stack Engineer (APIs, DB, caching).',\n",
    "    ]\n",
    "    hr = {\n",
    "        'resume_guidance': resume,\n",
    "        'job_description': jd,\n",
    "        'job_resume_match': match,\n",
    "        'recruiting_strategy': recruiting,\n",
    "        'ats_keywords': ats,\n",
    "    }\n",
    "    # Router/safety\n",
    "    salary = [\n",
    "        'What is the typical salary for a Solutions Architect in Stockholm?',\n",
    "        'Average pay for Data Scientist in Singapore?',\n",
    "        'How much does a DevOps engineer earn in Berlin?',\n",
    "        'Typical compensation for Backend Engineer in New York?'\n",
    "    ]\n",
    "    non_career = {\n",
    "        'cooking': ['How to bake a chocolate cake?', 'Recipe for paneer tikka?'],\n",
    "        'travel': ['2-day itinerary for Paris?', 'Best time to visit Kyoto?'],\n",
    "        'weather': ['What is the weather in Delhi today?', 'Is it raining in London?'],\n",
    "        'small_talk': ['Hello! How is your day?', 'Good morning!'],\n",
    "        'general_qna': ['How to set up Docker on Ubuntu?', 'Explain Git rebase vs merge.']\n",
    "    }\n",
    "    return hr, salary, non_career\n",
    "\n",
    "HR_PROMPTS, SALARY_TESTS, NON_CAREER = make_hr_prompts()\n",
    "sum_total = sum(len(v) for v in HR_PROMPTS.values())\n",
    "print('HR prompts total:', sum_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Generation Tests + CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, time\n",
    "def generate_once(domain, prompt, temperature=0.3, max_new_tokens=200):\n",
    "    chat = build_chatml(domain, prompt)\n",
    "    x = tokenizer([chat], return_tensors='pt').to(DEVICE)\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**x, max_new_tokens=max_new_tokens, temperature=temperature, do_sample=False)\n",
    "    dt = (time.time()-t0)*1000\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=False)\n",
    "    try:\n",
    "        resp = text.split('<|im_start|>assistant\n',1)[1].split('<|im_end|>')[0].strip()\n",
    "    except Exception:\n",
    "        resp = ''\n",
    "    ok, issues = VALIDATORS[domain](resp)\n",
    "    return {'domain': domain, 'prompt': prompt, 'ok': ok, 'issues': ';'.join(issues), 'len': len(resp.split()), 'preview': resp[:180], 'latency_ms': int(dt)}\n",
    "\n",
    "def run_local_suite(save_csv=True, drive_path='/content/drive/MyDrive/colab_training'):\n",
    "    rows=[]\n",
    "    for d, plist in HR_PROMPTS.items():\n",
    "        print(f'\n== {d} ==')\n",
    "        passed=0\n",
    "        for p in plist:\n",
    "            r = generate_once(d, p)\n",
    "            rows.append({'source':'local', **r})\n",
    "            passed += int(r['ok'])\n",
    "        rate = passed/len(plist)*100\n",
    "        print(f"{passed}/{len(plist)} = {rate:.1f}%")\n",
    "    # Save CSV\n",
    "    if save_csv:\n",
    "        local_csv = '/content/colab_training/eval_results_local.csv'\n",
    "        os.makedirs('/content/colab_training', exist_ok=True)\n",
    "        with open(local_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "            w=csv.DictWriter(f, fieldnames=rows[0].keys()); w.writeheader(); w.writerows(rows)\n",
    "        print('Saved:', local_csv)\n",
    "        # Copy to Drive if available\n",
    "        try:\n",
    "            if os.path.exists(drive_path):\n",
    "                drive_csv = os.path.join(drive_path, 'eval_results_local.csv')\n",
    "                import shutil; shutil.copy(local_csv, drive_csv)\n",
    "                print('Saved to Drive:', drive_csv)\n",
    "        except Exception as e:\n",
    "            print('Drive copy skipped:', e)\n",
    "    return rows\n",
    "\n",
    "_ = run_local_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint Tests: HR + Router/Safety + CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "ENDPOINT_ID = ''  # e.g., 'c53htbvf...'\n",
    "API_KEY = ''\n",
    "BASE = f'https://api.runpod.ai/v2/{ENDPOINT_ID}/runsync' if ENDPOINT_ID else None\n",
    "\n",
    "def call_endpoint(prompt, max_tokens=200, enable_validation=True, block_low_trust=True):\n",
    "    if not BASE: return None, None, 'no-endpoint'\n",
    "    body = {\n",
    "        'input': {\n",
    "            'prompt': prompt,\n",
    "            'sampling_params': {'max_tokens': max_tokens, 'temperature': 0.3},\n",
    "            'enable_validation': enable_validation,\n",
    "            'block_low_trust_intents': block_low_trust\n",
    "        }\n",
    "    }\n",
    "    headers={'Authorization': f'Bearer {API_KEY}', 'Content-Type':'application/json'}\n",
    "    resp = requests.post(BASE, headers=headers, json=body, timeout=120)\n",
    "    status = resp.status_code\n",
    "    j = {}\n",
    "    try:\n",
    "        j = resp.json()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return j, status, None\n",
    "\n",
    "def run_endpoint_suite(save_csv=True, drive_path='/content/drive/MyDrive/colab_training'):\n",
    "    if not BASE:\n",
    "        print('Skipping endpoint tests: set ENDPOINT_ID & API_KEY')\n",
    "        return []\n",
    "    rows=[]\n",
    "    # HR domains\n",
    "    for d, plist in HR_PROMPTS.items():\n",
    "        print(f'\n== Endpoint HR: {d} ==')\n",
    "        passed=0\n",
    "        for p in plist[:10]:  # limit to 10 per domain for speed\n",
    "            j, status, err = call_endpoint(p)\n",
    "            text = (((j or {}).get('output') or {}).get('choices') or [{}])[0].get('text','')\n",
    "            ok, issues = VALIDATORS[d](text)\n",
    "            rows.append({'source':'endpoint', 'domain': d, 'prompt': p, 'ok': ok, 'issues': ';'.join(issues), 'len': len(text.split()), 'preview': text[:180], 'latency_ms': (j or {}).get('executionTime', -1), 'http_status': status})\n",
    "            passed += int(ok)\n",
    "        rate = passed/max(1,len(plist[:10]))*100\n",
    "        print(f"{passed}/{len(plist[:10])} = {rate:.1f}%")\n",
    "    # Router/safety: salary blocking\n",
    "    print('\n== Router/Safety: Salary blocking ==')\n",
    "    for q in SALARY_TESTS:\n",
    "        j, status, err = call_endpoint(q, enable_validation=True, block_low_trust=True)\n",
    "        blocked = (j or {}).get('output', {}).get('blocked') or (j or {}).get('blocked')\n",
    "        rows.append({'source':'endpoint', 'domain': 'salary_block', 'prompt': q, 'ok': bool(blocked), 'issues': '' if blocked else 'not_blocked', 'len': 0, 'preview': json.dumps((j or {}).get('output', {}))[:180], 'latency_ms': (j or {}).get('executionTime', -1), 'http_status': status})\n",
    "        print('Blocked' if blocked else 'Not blocked', '|', q)\n",
    "    # Non-career routing (should not use career template)\n",
    "    print('\n== Router: Non-career domains ==')\n",
    "    for cat, plist in NON_CAREER.items():\n",
    "        for p in plist:\n",
    "            j, status, err = call_endpoint(p, enable_validation=True, block_low_trust=True)\n",
    "            text = (((j or {}).get('output') or {}).get('choices') or [{}])[0].get('text','')\n",
    "            rows.append({'source':'endpoint', 'domain': f'router_{cat}', 'prompt': p, 'ok': True, 'issues': '', 'len': len(text.split()), 'preview': text[:180], 'latency_ms': (j or {}).get('executionTime', -1), 'http_status': status})\n",
    "            print(cat, '|', text[:80].replace('\n',' '))\n",
    "    # Save CSV\n",
    "    if save_csv and rows:\n",
    "        local_csv = '/content/colab_training/eval_results_endpoint.csv'\n",
    "        os.makedirs('/content/colab_training', exist_ok=True)\n",
    "        with open(local_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "            w=csv.DictWriter(f, fieldnames=rows[0].keys()); w.writeheader(); w.writerows(rows)\n",
    "        print('Saved:', local_csv)\n",
    "        try:\n",
    "            if os.path.exists(drive_path):\n",
    "                drive_csv = os.path.join(drive_path, 'eval_results_endpoint.csv')\n",
    "                import shutil; shutil.copy(local_csv, drive_csv)\n",
    "                print('Saved to Drive:', drive_csv)\n",
    "        except Exception as e:\n",
    "            print('Drive copy skipped:', e)\n",
    "    return rows\n",
    "\n",
    "_ = run_endpoint_suite()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

